{"cells":[{"cell_type":"markdown","metadata":{"id":"jzyYRubKJ15A"},"source":["\n","\n","# Exercicio - SVM - Support Vector Machines"]},{"cell_type":"markdown","metadata":{"id":"wUrHEZ4bJ15D"},"source":["Neste tutorial, vamos trabalhar com o classificador SVM, ou Máquina de Vetores de Suporte. Um modelo muito usado em Machine Learning e capaz de lidar com problemas linear e não linearmente separáveis."]},{"cell_type":"markdown","metadata":{"id":"UCQJ7YdRJ15E"},"source":["Assim como nos exemplos anteriores, vamos nos prender em dataset bi-dimensionais, pois isso nos permitirá uma ampla visualização do que acontece por baixo dos panos nesses classificadores. Poderemos visualizar os pontos no espaço de busca/classificação e as superfícies de decisão. No final do tutorial aplicaremos esses métodos a problemas com mais dimensões. Vocês verão que o código é exatamente o mesmo para estes casos."]},{"cell_type":"markdown","metadata":{"id":"VQeo6VKgJ15F"},"source":["Imagine que estejam construindo uma rua em seu bairro e o prefeito quer que ela seja o mais larga possível, de forma a separar as casas do lado direito e esquerdo da rua. O que o SVM busca fazer é muito similar a isso, encontrar as margens mais disnte possíveis entre si que separem os exemplos de cada uma das classes. A reta no centro destas margens (ou seja, a faixa que separa as duas mãos da nossa rua), é a nossa superfície de decisão."]},{"cell_type":"markdown","metadata":{"id":"WMXhv2RvJ15G"},"source":["Observe a imagem abaixo, extraída do livro Introduction to Statistical Learning. Neste caso desejamos separar os pontos da classe azul dos pontos da classe roxa. As duas retas pontilhadas são as margens de cada classe (que o SVM busca aprender), enquanto a reta contínua é nossa superície de decisão."]},{"cell_type":"markdown","metadata":{"id":"axYhAUuUJ15H"},"source":["Antes da explicação matemática do SVM, vamos sentir o gostinho de executá-lo com o Scikit Learning utilizando um dataset clássico, o Iris. Neste modelo vamos classificar se uma íris (flor) é do tipo Setosa ou Versicolor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WsoQpR4ZJ15I"},"outputs":[],"source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oALUiYv4J15L"},"outputs":[],"source":["iris = datasets.load_iris()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fgVyF0gNJ15P"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-b8a3280719d6\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#dimensoes da petala\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#selecionamos apenas os dois tipos que temos interesse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"]}],"source":["iris = datasets.load_iris()\n","X = iris[\"data\"]  #dimensoes da petala\n","y = iris[\"target\"]\n","\n","#selecionamos apenas os dois tipos que temos interesse\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KIsPf4vTJ15R"},"outputs":[],"source":["svm_clf = SVC(C=100, kernel='linear')\n","svm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BNcy6LYkJ15U"},"outputs":[],"source":["svm_clf.predict([[2.5, 1.1]])"]},{"cell_type":"markdown","metadata":{"id":"NeGhJukZJ15X"},"source":["Vamos visualizar esses dados para entender melhor o comportamento do nosso classificador."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XvkbXwxzJ15Y"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jzDs4m5FJ15a"},"outputs":[],"source":["def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n","    w = svm_clf.coef_[0]\n","    b = svm_clf.intercept_[0]\n","\n","    # a reta de nossa superfície de decisão tem como equacao w0*x0 + w1*x1 + b = 0, logo\n","    # x1 = -w0/w1 * x0 - b/w1\n","    x0 = np.linspace(xmin, xmax, 200)\n","    decision_boundary = -w[0]/w[1] * x0 - b/w[1] #todos os valores de x1 em funcao dos valores de x0 na sup de decisao\n","\n","    margin = 1/w[1] #estamos desenhando a superficie em funcao de x1, logo a margem também sera em funcao dele, por isso divido por 1/w[1], que é o deslocamento da superficie de decisao para as maregsn\n","    margin_up = decision_boundary + margin\n","    margin_down = decision_boundary - margin\n","\n","    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n","    plt.plot(x0, margin_up, \"k--\", linewidth=2)\n","    plt.plot(x0, margin_down, \"k--\", linewidth=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IPjPqTlOJ15c"},"outputs":[],"source":["plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"upper left\", fontsize=14)\n","plot_svc_decision_boundary(svm_clf, 0, 5.5)\n","plt.axis([0, 5.5, 0, 2])\n"]},{"cell_type":"markdown","metadata":{"id":"xYX4MP-7J15f"},"source":["Note que o nosso modelo conseguiu aprender um hiperplano de separação entre as duas classes, que fica exatamente no centro das duas margens máximas encontradas por ele. Desta forma a, superfície de decisão encontrada é aquela que fica o mais distante possível das duas classes."]},{"cell_type":"markdown","metadata":{"id":"zCOkqP1eJ15f"},"source":["Observe que o parâmetro C=100 foi empregado no nosso modelo. Essa parâmtro indica o quão rigoroso eu desejo que o modelo seja em relação a violações à margem, ou seja, a pontos que fiquem do lado oposto à margem encontrada para sua classe. Isto é importante pois em alguns casos pode ser que não seja possível encontrar uma margem que separe todos os pontos, ou, se essa margem existir, ela seja muito pequena, tornando a generalização do modelo para novos exemplos muito ruim. Um outro problema que pode ser causado com um hiperparâmetro muito rigoroso é gerar uma grande sensibilidade a outliers."]},{"cell_type":"markdown","metadata":{"id":"qZA2X9ncJ15h"},"source":["Observe como ficaria nosso modelo com um parâmetro C=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zLinYiP8J15i"},"outputs":[],"source":["svm_clf = SVC(C=0.1, kernel='linear')\n","svm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rvoAKaYwJ15k"},"outputs":[],"source":["plt.figure(figsize=(12,2.7))\n","\n","plt.subplot(121)\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"upper left\", fontsize=14)\n","plot_svc_decision_boundary(svm_clf, 0, 5.5)\n","plt.axis([0, 5.5, 0, 2])"]},{"cell_type":"markdown","metadata":{"id":"yijwIrudJ15m"},"source":["Note que neste caso estamos sendo mais permissivos, o que nos gera margens maiores."]},{"cell_type":"markdown","metadata":{"id":"1E6u5unbJ15n"},"source":["Observe o que aconteceria se houvesse um outlier no nosso modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"z4D2Q_CZJ15n"},"outputs":[],"source":["X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\n","y_outliers = np.array([0, 0])\n","X1_with_outlier = np.concatenate([X, X_outliers[:1]], axis=0)\n","y1_with_outlier = np.concatenate([y, y_outliers[:1]], axis=0)\n","X2_with_outlier = np.concatenate([X, X_outliers[1:]], axis=0)\n","y2_with_outlier = np.concatenate([y, y_outliers[1:]], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hOMW4BTIJ15p"},"outputs":[],"source":["svm_clf = SVC(kernel=\"linear\", C=10**9)\n","svm_clf.fit(X2_with_outlier, y2_with_outlier)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"67jv8ER8J15s"},"outputs":[],"source":["plt.plot(X2_with_outlier[:, 0][y2_with_outlier==1], X2_with_outlier[:, 1][y2_with_outlier==1], \"bs\")\n","plt.plot(X2_with_outlier[:, 0][y2_with_outlier==0], X2_with_outlier[:, 1][y2_with_outlier==0], \"yo\")\n","plot_svc_decision_boundary(svm_clf, 0, 5.5)\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.annotate(\"Outlier\",\n","             xy=(X_outliers[1][0], X_outliers[1][1]),\n","             xytext=(3.2, 0.08),\n","             ha=\"center\",\n","             arrowprops=dict(facecolor='black', shrink=0.1),\n","             fontsize=16,\n","            )\n","plt.axis([0, 5.5, 0, 2])"]},{"cell_type":"markdown","metadata":{"id":"R-o6hOkZJ15u"},"source":["Note como neste caso nossa margem se tornou extramemente pequena devido apenas a um exemplo. Isto pode causar erros de classificação durante o processo de produção, pois o modelo não conseguirá classificar corretamente um ponto da classe azul que esteja mais mais deslocado sentido classe amararela."]},{"cell_type":"markdown","metadata":{"id":"YntuUq7wJ15u"},"source":["Uma outra técnica importante de ser empregada com o SVM é escalar (normalizar subtraindo a média e dividindo pelo desvio padrão) os dados. Este método é muito sensível à escala, uma vez que busca maximizar a margem de separação utilizando uma métrica de distância do ponto à margem, ele provavelmente irá negligenciar features com pequenas dimensões, enquanto features com valores altos dominarão o cálculo da distância. Veja o exemplo abaixo:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MKfhl_3TJ15v"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OwPpTBI8J15x"},"outputs":[],"source":["X_new = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n","y_new = np.array([0, 0, 1, 1])\n","\n","svm_clf2 = SVC(kernel=\"linear\", C=100)\n","svm_clf2.fit(X_new, y_new)\n","\n","plt.figure(figsize=(12,3.2))\n","plt.subplot(121)\n","plt.plot(X_new[:, 0][y_new==1], X_new[:, 1][y_new==1], \"bo\")\n","plt.plot(X_new[:, 0][y_new==0], X_new[:, 1][y_new==0], \"rs\")\n","plot_svc_decision_boundary(svm_clf2, 0, 6)\n","plt.xlabel(\"$x_0$\", fontsize=20)\n","plt.ylabel(\"$x_1$  \", fontsize=20, rotation=0)\n","plt.title(\"Sem normalização\", fontsize=16)\n","plt.axis([0, 6, 0, 90])\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X_new)\n","svm_clf2.fit(X_scaled, y_new)\n","\n","plt.subplot(122)\n","plt.plot(X_scaled[:, 0][y_new==1], X_scaled[:, 1][y_new==1], \"bo\")\n","plt.plot(X_scaled[:, 0][y_new==0], X_scaled[:, 1][y_new==0], \"rs\")\n","plot_svc_decision_boundary(svm_clf2, -2, 2)\n","plt.xlabel(\"$x_0$\", fontsize=20)\n","plt.title(\"Com normalização\", fontsize=16)\n","plt.axis([-2, 2, -2, 2])"]},{"cell_type":"markdown","metadata":{"id":"Iw4RPjUPJ150"},"source":["Observe que a escala da feature $x_1$ na imagem da esquerda é muito maior que a da feature $x_0$, e que a margem do modelo sem normalização é significativamente menor (em proporção) e horizontal, uma vez que $x_1$ está dominando o cálculo da distância.Note também que no segundo caso mais pontos participam da margem (são vetores de suporte), já que a feature $x_0$ passa a contribuir igualmente para o cálculo de distânca."]},{"cell_type":"markdown","metadata":{"id":"q2f6wNFKJ150"},"source":["No exemplo utilizando o dataset Iris, acima, não normalizamos o dados apenas para fins didádicos de facilitar a visualização."]},{"cell_type":"markdown","metadata":{"id":"M82hfIQRJ152"},"source":["\u003ch5\u003eComo o SVM classifica os dados?\u003c/h5\u003e"]},{"cell_type":"markdown","metadata":{"id":"uXwTxoljJ152"},"source":["Matematicamente, após treinado o modelo, a tarefa de classificação do SVM é bem simples e segue de acordo com a equação abaixo:"]},{"cell_type":"markdown","metadata":{"id":"BeIaV2FAJ153"},"source":["$\n","\\begin{equation}\n","  y^{(i)}_{predito} ==\\left\\{\n","  \\begin{array}{@{}ll@{}}\n","    0, \u0026 \\text{se}\\ w_1x_1 + w_2x_2 + \\dots + w_mx_m + b \u003c  0 \\\\\n","    1, \u0026 \\text{se}\\ w_1x_1 + w_2x_2 + \\dots + w_mx_m + b \\geq 0\n","  \\end{array}\\right.\n","\\end{equation}\n","$"]},{"cell_type":"markdown","metadata":{"id":"nI4mt-QuJ155"},"source":["Ou, de forma vetorial"]},{"cell_type":"markdown","metadata":{"id":"wySAzIXtJ155"},"source":["$\n","\\begin{equation}\n","  \\hat{y} ==\\left\\{\n","  \\begin{array}{@{}ll@{}}\n","    0, \u0026 \\text{se}\\ W^T\\cdot  x + b \u003c  0 \\\\\n","    1, \u0026 \\text{se}\\ W^T\\cdot x + b \\geq  0  \n","  \\end{array}\\right.\n","\\end{equation}\n","$"]},{"cell_type":"markdown","metadata":{"id":"eDqrRJ19J156"},"source":["onde $W$ e $b$ são os coeficientes da superfície de decisão, encontrados pelo SVM."]},{"cell_type":"markdown","metadata":{"id":"MgcPRE8qJ156"},"source":["Vamos ver um exemplo para os pontos $x_1 = (1.5, 0.7)$ e $x_2 = (4.1, 1.1)$, em destaque no gráfico seguinte."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uAS3DXPgJ157"},"outputs":[],"source":["x1 = [1.5, 0.7]\n","x2 = [4.1, 1.1]\n","\n","svm_clf = SVC(kernel=\"linear\", C=100)\n","svm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0V7OFu3FJ159"},"outputs":[],"source":["plt.figure(figsize=(12,2.7))\n","\n","plt.subplot(121)\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n","plt.scatter(x1[0], x1[1], s=50, alpha=0.5, c=\"g\")\n","plt.scatter(x2[0], x2[1], s=50, alpha=0.5, c=\"m\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"upper left\", fontsize=14)\n","plot_svc_decision_boundary(svm_clf, 0, 5.5)\n","plt.axis([0, 5.5, 0, 2])"]},{"cell_type":"markdown","metadata":{"id":"g8b1s1n-J15_"},"source":["Vamos implementar manualmente o cálculo de classificação do SVM, porém com o Scikit podemos obter a classe rapidamente utilizando o método predict()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"86MZRn-tJ16A"},"outputs":[],"source":["score = np.dot(svm_clf.coef_, x1) + svm_clf.intercept_\n","classe = 1 if score \u003e 0 else 0\n","print(\"O ponto \" + str(x1) + \" pertence à classe \" + str(classe) + \" com score \" + str(score[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-zlZ_2UkJ16C"},"outputs":[],"source":["score = np.dot(svm_clf.coef_, x2) + svm_clf.intercept_\n","classe = 1 if score \u003e 0 else 0\n","print(\"O ponto \" + str(x2) + \" pertence à classe \" + str(classe) + \" com score \" + str(score[0]))"]},{"cell_type":"markdown","metadata":{"id":"RT3Upe93J16E"},"source":["Mas matematicamente, o que isto quer dizer?"]},{"cell_type":"markdown","metadata":{"id":"ypKeD_pjJ16E"},"source":["A nossa função de decisão $h = W^T\\cdot x + b$ pode ser visualizada no gráfico abaixo (plano azul hachurado). Quando calculamos o valor de $h$ estamos gerando um score que obedece a regra de classificação do SVM (foi a partir desta regra que ele encontrou os parâmetros $W$) de acordo com as posição do exemplo em questão no espaço multidimensional. Observe que nossa superfície de decisão é o plano onde $h =\n","0$ e a nosso limite de decisão (decision boundery) é o hiperplano (reta no nosso caso) onde as duas regiões em destaque se cruzam."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7qo9oTMFJ16F"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","def plot_3D_decision_function(ax, w, b, x1_lim=[0, 5.0], x2_lim=[0, 2.0], plot_margin=True):\n","    x1_in_bounds = (X[:, 0] \u003e x1_lim[0]) \u0026 (X[:, 0] \u003c x1_lim[1])\n","    X_crop = X[x1_in_bounds]\n","    y_crop = y[x1_in_bounds]\n","    x1s = np.linspace(x1_lim[0], x1_lim[1], 40)\n","    x2s = np.linspace(x2_lim[0], x2_lim[1], 40)\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","    xs = np.c_[x1.ravel(), x2.ravel()]\n","    df = (xs.dot(w) + b).reshape(x1.shape)\n","    m = 1 / np.linalg.norm(w)\n","    if plot_margin:\n","        boundary_x2s = -x1s*(w[0]/w[1])-b/w[1]\n","        margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1]\n","        margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1]\n","        boundary_in_bounds = (boundary_x2s \u003e x1_lim[0]) \u0026 (boundary_x2s \u003c= x2_lim[1])\n","        m1_in_bounds = (margin_x2s_1 \u003e x1_lim[0]) \u0026 (margin_x2s_1 \u003c x2_lim[1])\n","        m2_in_bounds = (margin_x2s_2 \u003e x1_lim[0]) \u0026 (margin_x2s_2 \u003c x2_lim[1])\n","        ax.plot(x1s[boundary_in_bounds], boundary_x2s[boundary_in_bounds], 0, \"k-\", linewidth=2, label=r\"Decisão\")\n","        ax.plot(x1s[m1_in_bounds], margin_x2s_1[m1_in_bounds], 0, \"k--\", linewidth=2, label=r\"Margem\")\n","        ax.plot(x1s[m2_in_bounds], margin_x2s_2[m2_in_bounds], 0, \"k--\", linewidth=2)\n","    scores_1 = svm_clf.decision_function(X_crop[y_crop==1])\n","    scores_2 = svm_clf.decision_function(X_crop[y_crop==0])\n","    ax.plot_surface(x1, x2, df, alpha=0.1, color=\"b\")\n","    ax.plot_wireframe(x1, x2, df, alpha=0.05, color=\"k\")\n","    ax.plot_surface(x1s, x2, np.zeros_like(x1),\n","                color=\"g\", alpha=0.2)\n","    ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], scores_1, \"bs\")\n","    ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], scores_2, \"yo\")\n","    ax.axis(x1_lim + x2_lim)\n","    ax.set_xlabel(r\"Petal length\", fontsize=15)\n","    ax.set_ylabel(r\"Petal width\", fontsize=15)\n","    ax.set_zlabel(r\"$h = \\mathbf{w}^T \\mathbf{x} + b$\", fontsize=18)\n","    ax.legend(loc=\"upper left\", fontsize=16)\n","\n","\n","fig = plt.figure(figsize=(20, 6))\n","ax1 = fig.add_subplot(121, projection='3d')\n","ax1.elev = 40\n","plot_3D_decision_function(ax1, w=svm_clf.coef_[0], b=svm_clf.intercept_[0], plot_margin=True)\n","\n","ax2 = fig.add_subplot(122, projection='3d')\n","ax2.elev = 10\n","ax2.azim = 100\n","plot_3D_decision_function(ax2, w=svm_clf.coef_[0], b=svm_clf.intercept_[0], plot_margin=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-IeskaBSJ16H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O7XvbUx9J16J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"E91zYflKJ16L"},"source":["\u003ch3\u003eObtendo probabilidades\u003c/h3\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hJL8BS9aJ16M"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SRDfjksvJ16O"},"outputs":[],"source":["svm_clf_probs = SVC(kernel=\"linear\", probability=True)\n","svm_clf_probs.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2vI7AssCJ16Q"},"outputs":[],"source":["svm_scores = svm_clf_probs.decision_function(X)\n","svm_scores_expanded = np.expand_dims(svm_scores, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-sQ63gx5J16S"},"outputs":[],"source":["lr = LogisticRegression()\n","lr.fit(svm_scores_expanded, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mU8_H-yCJ16T"},"outputs":[],"source":["lr.predict_proba(svm_scores_expanded)[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"t3-6cY06J16V"},"outputs":[],"source":["svm_clf_probs.predict_proba(X)[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"x_r33HbgJ16X"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bfUh_tbTJ16a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zEOYgbc4J16c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wpun2omaJ16d"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"TRK5jQHAJ16f"},"source":["O SVM funciona muito bem em muitos casos, porém quase todos os datasets não são linearmente separáveis. Uma abordagem para lidar com este problema é a utilização de features polinomiais, assim como fazemos com regressão linear (polinomial). Veja o exemplo abaixo. Como separar estes dados utilizando o SVM?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eoCaP9mOJ16f"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dAV6IDCnJ16h"},"outputs":[],"source":["c1 = np.array([-4, -3, 3, 4])\n","c2 = np.array([-2, -1, 0, 1, 2])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ca-cx-QbJ16l"},"outputs":[],"source":["plt.scatter(x=c1, y=np.zeros_like(c1), c='blue')\n","plt.scatter(x=c2, y=np.zeros_like(c2), c='red')\n","plt.yticks([])"]},{"cell_type":"markdown","metadata":{"id":"l7-bJnShJ16o"},"source":["Se aumentarmos uma dimensão no nosso espaço, adicionando uma feature $x_2 = x^2$, projetamos nossos dados em um espaço onde eles são linearmente separáveis."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1kwUsR4qJ16o"},"outputs":[],"source":["c1_2 = c1**2\n","c2_2 = c2**2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5uXXKrSUJ16q"},"outputs":[],"source":["plt.scatter(x=c1, y=c1_2, c='blue')\n","plt.scatter(x=c2, y=c2_2, c='red')\n","plt.axhline(y=6, ls='--', c='gray')"]},{"cell_type":"markdown","metadata":{"id":"fyw2nGHtJ16r"},"source":["Utilizando o Scikit-Learn conseguimos implementar o SVM com essa característica facilmente, é só adicionarmos o PolynomialFeatures a nosso pipeline:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rkuGP0npJ16r"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QrW3d3HqJ16u"},"outputs":[],"source":["polynomial_svm_clf = Pipeline([\n","    (\"poly_fetaures\", PolynomialFeatures(degree=3)),\n","    (\"scaler\", StandardScaler()),\n","    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Acgw-Ko9J16w"},"outputs":[],"source":["polynomial_svm_clf.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"SRcogypwJ16y"},"source":["Adicionar features polinomiais funciona muito bem na maioris dos algoritmos de Machine Learning (não apenas não apenas no SVM), porém um polinômio de baixo grau não é capaz de lidar com problemas complexos e polinômios de graus muito altos geram um número muito grande de features, prejudicando a performance do modelo. O svm nos permite aplicar quase todas as técnincas matemáticas sem necessariamente adicionarmos dimensões a nosso espaço, apenas modificando as métricas de distância/similaridade por meio dos \u003cb\u003ekernel tricks\u003c/b\u003e, evitando a explosão combinatória do número de features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CK3tSsHqJ16z"},"outputs":[],"source":["from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pn7YFH82J160"},"outputs":[],"source":["poly_kernel_svm_clf = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yki1PRzsJ161"},"outputs":[],"source":["poly_kernel_svm_clf.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"CSV9hl8KJ163"},"source":["Uma outra forma de lidarmos com problemas não lineares é criarmos features de semelhança computadas por meio de uma função de similaridade. A Gaussian Radial Bases Function (RBF) é um exemplo de função de similaridade :\n"]},{"cell_type":"markdown","metadata":{"id":"CJ27E-Y0J163"},"source":["$\\phi\\gamma(x, \\ell) = exp(-\\gamma\\|x-\\ell\\|^2)$"]},{"cell_type":"markdown","metadata":{"id":"-dxQiPaYJ164"},"source":["que é uma função gaussiana (simétrica em forma de sino) que assume valores entra 0 e 1"]},{"cell_type":"markdown","metadata":{"id":"iCAnGPsKJ165"},"source":["Voltando ao nosso primeiro conjunto de dados:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AUr5lbctJ165"},"outputs":[],"source":["x1 = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C3Baw-wVJ167"},"outputs":[],"source":["plt.scatter(x=c1, y=np.zeros_like(c1), c='blue')\n","plt.scatter(x=[-1], y=[0], s=150, alpha=0.5, c=\"m\")\n","plt.scatter(x=c2, y=np.zeros_like(c2), c='red')\n","plt.yticks([])"]},{"cell_type":"markdown","metadata":{"id":"hnSyKA_1J168"},"source":["Vamos analisar este exemplo em relação a dois pontos de referência, $\\ell_1 = -2$ e $\\ell_2 = 1$. O exemplo $x_1 = -1$ (em destaque) está originalmente localizado a uma distância 1 da primeira referência e 2 da segunda. Porém suas novas features são $x_{1,1} =exp(-0.3\\times 1^2) \\approx 0.74$ e $x_{1,2} =exp(-0.3\\times 2^2) \\approx 0.30$ Como pode ser visto no gráfico abaixo, com isso elas são linearmente separáveis."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MI-WdSydJ169"},"outputs":[],"source":["def gaussian_rbf(x, ell, gamma=0.3):\n","    return np.exp(-gamma * np.linalg.norm(x-ell)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZX6q8CVZJ16_"},"outputs":[],"source":["c1_g1 = np.array([gaussian_rbf(x,-2) for x in c1])\n","c1_g2 = np.array([gaussian_rbf(x, 1) for x in c1])\n","c2_g1 = np.array([gaussian_rbf(x,-2) for x in c2])\n","c2_g2 = np.array([gaussian_rbf(x, 1) for x in c2])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pcH6XWrEJ17B"},"outputs":[],"source":["x1_g = gaussian_rbf(x1, -2)\n","x2_g = gaussian_rbf(x1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wvv11tJUJ17C"},"outputs":[],"source":["plt.scatter(x=c1_g1, y=c1_g2, c='blue')\n","plt.scatter(x=[x1_g], y=[x2_g], s=150, alpha=0.5, c=\"m\")\n","plt.scatter(x=c2_g1, y=c2_g2, c='red')\n","x1, y1 = [-0.1, 1.1], [0.57, -0.1]\n","plt.plot(x1, y1, c='gray', ls='--')\n","plt.axis([-0.1, 1.1, -0.1, 1.1])"]},{"cell_type":"markdown","metadata":{"id":"TtVyjB61J17F"},"source":["Note que com a utilização de kernels não adicionamos dimensões a nosso espaço, mas sim computamos novas features de acordo com uma função de kernel."]},{"cell_type":"markdown","metadata":{"id":"CVtolAdpJ17G"},"source":["Observe que agora as amostras são linearmente separáveis. Neste caso nos perguntamos como escolher os pontos de referência. Geralmente esses pontos são calculados como todos os pontos do dataset, ou seja, cada exemplo se torna um ponto de referência e calculamos a RBF para todos."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6tVCruumJ17G"},"outputs":[],"source":["rbf_kernel_svm_clf = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('svm_clf', SVC(kernel='rbf', C=5))\n","])\n","rbf_kernel_svm_clf.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"7GAaXz0gJ17H"},"source":["É importante destacar que quando utilizamos as funções de kernel nós não adicionamos features ao nosso conjunto de dados, mas sim calculamos uma distância \"similuando\" que estes dados tenham sido mapeados a num novo espaço. O gráfico acima apenas ilustra a distancia entre os pontos utilizando o kernel rbf em relação aos pontos de referência $\\ell_1$ e $\\ell_2$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Sl26jtwIJ17I"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0Sjp1djgJ17J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XevcL4P2J17L"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5JcRxf3pJ17M"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f4aWmrOGJ17O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sAkGnYMsJ17R"},"source":["\u003ch3\u003eSVM Regression\u003c/h3\u003e"]},{"cell_type":"markdown","metadata":{"id":"3ZGN1NjfJ17R"},"source":["Também é possível utilizarmos o SVM para problemas de regressão. Neste caso, o SVM busca coeficientes que minimizem uma loss onde apenas os resíduos mair que uma contante pré definida contribuiem par a funçãso de loss."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2w9wxYc6J17S"},"outputs":[],"source":["from sklearn.svm import LinearSVR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J2dApCvVJ17V"},"outputs":[],"source":["svm_reg = LinearSVR(epsilon=1.5)\n","svm_reg.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"klTbks9LJ17W"},"source":["Assim como na regressão polinomial, podemos lidar com problemas não lineares pelo SVR, apenas se faz necessário o uso de kernels."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qe5dXwKjJ17X"},"outputs":[],"source":["from sklearn.svm import SVR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ju85GNr8J17Y"},"outputs":[],"source":["svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n","svm_reg.fit(X,y)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":0}